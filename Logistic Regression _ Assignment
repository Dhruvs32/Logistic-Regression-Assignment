{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO3B3yka1zxVoWI96dxMY+8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Question 1: What is Logistic Regression, and how does it differ from Linear Regression?\n","Answer 1:- Logistic Regression is a statistical and machine learning algorithm used to predict the probability that a given input belongs to a particular class, most commonly for binary classification problems (e.g., spam vs. not spam, disease vs. no disease).\n","Here’s the breakdown:\n","\n","1. Core Idea\n","Logistic regression models probabilities using the logistic (sigmoid) function:\n"," p=11+e−(β0+β1x1+⋯+βnxn)p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)}}p=1+e−(β0​+β1​x1​+⋯+βn​xn​)1​\n","Output: a value between 0 and 1, which can be interpreted as the probability of belonging to the positive class.\n","\n","\n","\n","2. How It Differs from Linear Regression\n","Feature\n","Linear Regression\n","Logistic Regression\n","Purpose\n","Predicts a continuous numeric value (e.g., price, height).\n","Predicts a probability for classification problems.\n","Output Range\n","−∞-\\infty−∞ to +∞+\\infty+∞\n","0 to 1 (via sigmoid/logistic function).\n","Equation\n","y=β0+β1x1+⋯+βnxny = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_nx_ny=β0​+β1​x1​+⋯+βn​xn​\n","p=11+e−(β0+β1x1+⋯+βnxn)p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\dots + \\beta_nx_n)}}p=1+e−(β0​+β1​x1​+⋯+βn​xn​)1​\n","Loss Function\n","Mean Squared Error (MSE)\n","Log Loss / Cross-Entropy Loss\n","Type of Problem\n","Regression (continuous outputs).\n","Classification (categorical outputs).\n","Linearity Assumption\n","Assumes output is a linear function of inputs.\n","Assumes log-odds are a linear function of inputs.\n","\n","\n","3. Why Logistic Regression Works for Classification\n","In binary classification, we want probabilities between 0 and 1.\n","\n","\n","If we used linear regression directly, predictions could be <0 or >1, which doesn’t make sense for probabilities.\n","\n","\n","The sigmoid function “squashes” the output into [0,1][0,1][0,1], making it suitable for classification.\n","\n","\n","Decision rule:\n"," Predict class 1 if p≥0.5,else class 0.\\text{Predict class 1 if } p \\ge 0.5, \\quad \\text{else class 0}.Predict class 1 if p≥0.5,else class 0.\n","\n","💡 Quick Analogy:\n","Linear regression is like predicting the exact temperature tomorrow.\n","\n","\n","Logistic regression is like predicting the chance it will rain tomorrow.\n","\n","\n","Question 2: Explain the role of the Sigmoid function in Logistic Regression.\n","Answer 2:- The sigmoid function is the mathematical “bridge” in logistic regression that turns a linear equation into a probability between 0 and 1.\n","\n","1. The Sigmoid Function Formula\n","σ(z)=11+e−z\\sigma(z) = \\frac{1}{1 + e^{-z}}σ(z)=1+e−z1​\n","Where:\n","z=β0+β1x1+⋯+βnxnz = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_nx_nz=β0​+β1​x1​+⋯+βn​xn​ (the linear combination of inputs).\n","\n","\n","eee is the base of the natural logarithm.\n","\n","\n","\n","2. Role in Logistic Regression\n","Transforms linear output to probability\n","\n","\n","The raw score zzz from the linear equation can be any value from −∞-\\infty−∞ to +∞+\\infty+∞.\n","\n","\n","The sigmoid “squashes” it into a range between 0 and 1, perfect for probability interpretation.\n","\n","\n","Enables classification\n","\n","\n","After computing the probability ppp, we can apply a threshold (commonly 0.5) to decide the class.\n"," Example:\n"," If p≥0.5⇒Class 1, else Class 0.\\text{If } p \\ge 0.5 \\Rightarrow \\text{Class 1, else Class 0}.If p≥0.5⇒Class 1, else Class 0.\n","Smooth and differentiable\n","\n","\n","The function’s smooth S-shape allows gradient-based optimization methods (like Gradient Descent) to work efficiently.\n","\n","\n","Maps log-odds to probability\n","\n","\n","Logistic regression models the log-odds of the dependent variable as a linear function of inputs.\n"," The sigmoid converts those log-odds into a probability.\n","\n","\n","\n","3. Visual Intuition\n","For very negative zzz → sigmoid output ≈ 0 (low probability of positive class).\n","\n","\n","For very positive zzz → sigmoid output ≈ 1 (high probability of positive class).\n","\n","\n","Around z=0z = 0z=0 → output ≈ 0.5 (maximum uncertainty).\n","\n","\n","\n","💡 Analogy: Think of the sigmoid as a “probability dial.”\n"," The linear regression part decides where to set the dial, and the sigmoid makes sure it never goes below 0 or above 1.\n","Question 3: What is Regularization in Logistic Regression and why is it needed?\n","Answer 3:- Regularization in logistic regression is a technique used to prevent overfitting by adding a penalty term to the loss function, which discourages the model from assigning excessively large weights to features.\n","\n","1. Why Regularization is Needed\n","In logistic regression, the model learns weights (β\\betaβ) that best separate the classes.\n","\n","\n","If the dataset has:\n","\n","\n","Noisy features\n","\n","\n","Too many features\n","\n","\n","Correlated features (multicollinearity)\n","\n","\n","… the model can overfit:\n","\n","\n","It will assign very large coefficients to some features just to perfectly fit the training data.\n","\n","\n","This leads to poor generalization (bad performance on unseen data).\n","\n","\n","Regularization controls model complexity by keeping coefficients small.\n","\n","2. How It Works\n","The standard logistic regression loss function (Log Loss / Cross-Entropy Loss) is:\n","Loss=−1m∑i=1m[yilog⁡(pi)+(1−yi)log⁡(1−pi)]\\text{Loss} = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]Loss=−m1​i=1∑m​[yi​log(pi​)+(1−yi​)log(1−pi​)]\n","Regularization adds a penalty term:\n","a) L2 Regularization (Ridge)\n","Loss=Log Loss+λ∑j=1nβj2\\text{Loss} = \\text{Log Loss} + \\lambda \\sum_{j=1}^{n} \\beta_j^2Loss=Log Loss+λj=1∑n​βj2​\n","Penalizes large weights quadratically.\n","\n","\n","Keeps all features but shrinks their influence.\n","\n","\n","b) L1 Regularization (Lasso)\n","Loss=Log Loss+λ∑j=1n∣βj∣\\text{Loss} = \\text{Log Loss} + \\lambda \\sum_{j=1}^{n} |\\beta_j|Loss=Log Loss+λj=1∑n​∣βj​∣\n","Penalizes absolute weight values.\n","\n","\n","Can shrink some weights to zero → useful for feature selection.\n","\n","\n","c) Elastic Net\n","Loss=Log Loss+λ1∑∣βj∣+λ2∑βj2\\text{Loss} = \\text{Log Loss} + \\lambda_1 \\sum |\\beta_j| + \\lambda_2 \\sum \\beta_j^2Loss=Log Loss+λ1​∑∣βj​∣+λ2​∑βj2​\n","Combination of L1 and L2.\n","\n","\n","\n","3. Benefits\n","Prevents overfitting by reducing variance.\n","\n","\n","Improves generalization to unseen data.\n","\n","\n","Helps with multicollinearity by distributing weights more evenly.\n","\n","\n","Performs feature selection in the case of L1.\n","\n","\n","\n","💡 Analogy:\n"," Without regularization, logistic regression is like a student memorizing every detail of the textbook (overfitting).\n"," With regularization, it’s like encouraging the student to focus only on the most important ideas (generalizing).\n","\n","\n","Question 4: What are some common evaluation metrics for classification models, and why are they important?\n","Answer 4:- In classification problems, evaluation metrics tell us how well our model is performing and whether it’s making the right kind of mistakes for our use case.\n","\n","1. Common Evaluation Metrics\n","a) Accuracy\n","Accuracy=Number of Correct PredictionsTotal Predictions\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Predictions}}Accuracy=Total PredictionsNumber of Correct Predictions​\n","Meaning: Percentage of correctly classified samples.\n","\n","\n","Good for: Balanced datasets.\n","\n","\n","Limitation: Misleading for imbalanced datasets (e.g., predicting all negatives in a 99:1 class ratio can still give 99% accuracy).\n","\n","\n","\n","b) Precision\n","Precision=True PositivesTrue Positives + False Positives\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}Precision=True Positives + False PositivesTrue Positives​\n","Meaning: Of all the predicted positives, how many were actually positive?\n","\n","\n","Use case: When false positives are costly (e.g., spam detection—don’t flag legitimate emails).\n","\n","\n","\n","c) Recall (Sensitivity / True Positive Rate)\n","Recall=True PositivesTrue Positives + False Negatives\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}Recall=True Positives + False NegativesTrue Positives​\n","Meaning: Of all actual positives, how many did we correctly identify?\n","\n","\n","Use case: When false negatives are costly (e.g., disease diagnosis—don’t miss sick patients).\n","\n","\n","\n","d) F1 Score\n","F1=2×Precision×RecallPrecision + Recall\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}F1=2×Precision + RecallPrecision×Recall​\n","Meaning: Harmonic mean of precision and recall.\n","\n","\n","Use case: When there’s an imbalance and you want a balance between precision and recall.\n","\n","\n","\n","e) ROC-AUC (Receiver Operating Characteristic – Area Under Curve)\n","ROC Curve: Plots True Positive Rate (Recall) vs. False Positive Rate at different thresholds.\n","\n","\n","AUC: Measures the model’s ability to rank positives higher than negatives.\n","\n","\n","Good for: Comparing models independently of classification threshold.\n","\n","\n","\n","f) Confusion Matrix\n","A table showing TP, TN, FP, FN counts.\n","\n","\n","Use case: Gives a complete breakdown of prediction outcomes, not just a single number.\n","\n","\n","\n","2. Why They’re Important\n","Different problems prioritize different errors:\n","\n","\n","Fraud detection → high recall.\n","\n","\n","Email spam filter → high precision.\n","\n","\n","Accuracy alone can be misleading for imbalanced data.\n","\n","\n","Helps in model selection: Choosing the metric aligned with business goals leads to better real-world performance.\n","\n","\n","Supports threshold tuning: Metrics like Precision-Recall and ROC help decide where to set probability cutoffs.\n","\n","\n","\n","💡 Quick Analogy:\n"," Accuracy is like saying “I was right most of the time,”\n"," Precision is “When I said yes, I was usually right,”\n"," Recall is “I rarely missed a yes,”\n"," F1 is “I balanced the two,”\n"," AUC is “I ranked things well overall.”\n"],"metadata":{"id":"sNF8fIF_S1KP"}},{"cell_type":"markdown","source":["Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy. (Use Dataset from sklearn package) (Include your Python code and output in the code box below.)\n","Answer 5:-# Import necessary libraries\n","import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset from sklearn\n","data = load_breast_cancer()\n","\n","# Convert to Pandas DataFrame\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# Split into features (X) and target (y)\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Create and train logistic regression model\n","model = LogisticRegression(max_iter=10000)  # increased iterations for convergence\n","model.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print accuracy\n","print(f\"Logistic Regression Model Accuracy: {accuracy:.4f}\")\n","\n","Output:- Logistic Regression Model Accuracy: 0.9561\n","\n","Question 6: Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy. (Use Dataset from sklearn package) (Include your Python code and output in the code box below.)\n","Answer 6:- # Import libraries\n","import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","data = load_breast_cancer()\n","\n","# Convert to Pandas DataFrame\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# Features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Create Logistic Regression model with L2 regularization (Ridge)\n","# C is the inverse of regularization strength — smaller C = stronger regularization\n","model = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=10000)\n","\n","# Train model\n","model.fit(X_train, y_train)\n","\n","# Predict\n","y_pred = model.predict(X_test)\n","\n","# Accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Output coefficients and accuracy\n","print(\"Model Coefficients:\")\n","for feature, coef in zip(X.columns, model.coef_[0]):\n","    print(f\"{feature}: {coef:.4f}\")\n","\n","print(f\"\\nIntercept: {model.intercept_[0]:.4f}\")\n","print(f\"\\nAccuracy: {accuracy:.4f}\")\n","Output:- Model Coefficients:\n","mean radius: 1.0274\n","mean texture: 0.2215\n","mean perimeter: -0.3621\n","mean area: 0.0255\n","mean smoothness: -0.1562\n","mean compactness: -0.2377\n","mean concavity: -0.5326\n","mean concave points: -0.2837\n","mean symmetry: -0.2267\n","mean fractal dimension: -0.0365\n","radius error: -0.0971\n","texture error: 1.3706\n","perimeter error: -0.1814\n","area error: -0.0872\n","smoothness error: -0.0225\n","compactness error: 0.0474\n","concavity error: -0.0429\n","concave points error: -0.0324\n","symmetry error: -0.0347\n","fractal dimension error: 0.0116\n","worst radius: 0.1117\n","worst texture: -0.5089\n","worst perimeter: -0.0156\n","worst area: -0.0169\n","worst smoothness: -0.3077\n","worst compactness: -0.7727\n","worst concavity: -1.4286\n","worst concave points: -0.5109\n","worst symmetry: -0.7469\n","worst fractal dimension: -0.1009\n","\n","Intercept: 28.6487\n","\n","Accuracy: 0.9561\n","\n","Question 7: Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report. (Use Dataset from sklearn package) (Include your Python code and output in the code box below.)\n","Answer:- # Import libraries\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","\n","# Load dataset\n","data = load_iris()\n","\n","# Convert to Pandas DataFrame\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# Features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Create logistic regression model with OvR strategy\n","model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=10000)\n","\n","# Train model\n","model.fit(X_train, y_train)\n","\n","# Predict\n","y_pred = model.predict(X_test)\n","\n","# Print classification report\n","print(\"Classification Report:\\n\")\n","print(classification_report(y_test, y_pred, target_names=data.target_names))\n","\n","Output:- Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","      setosa       1.00      1.00      1.00        10\n","  versicolor       1.00      0.89      0.94         9\n","   virginica       0.92      1.00      0.96        11\n","\n","    accuracy                           0.97        30\n","   macro avg       0.97      0.96      0.97        30\n","weighted avg       0.97      0.97      0.97        30\n","\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n","\n","Question 8: Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy. (Use Dataset from sklearn package) (Include your Python code and output in the code box below.)\n","Answer:- # Import libraries\n","import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","\n","# Load dataset\n","data = load_breast_cancer()\n","\n","# Convert to Pandas DataFrame\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# Features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Define parameter grid\n","param_grid = {\n","    'C': [0.01, 0.1, 1, 10, 100],           # Regularization strength\n","    'penalty': ['l1', 'l2'],                # Regularization type\n","    'solver': ['liblinear']                 # Solver that supports both L1 and L2\n","}\n","\n","# Create Logistic Regression model\n","log_reg = LogisticRegression(max_iter=10000)\n","\n","# Create GridSearchCV\n","grid_search = GridSearchCV(\n","    estimator=log_reg,\n","    param_grid=param_grid,\n","    cv=5,                  # 5-fold cross-validation\n","    scoring='accuracy',\n","    n_jobs=-1\n",")\n","\n","# Fit GridSearchCV\n","grid_search.fit(X_train, y_train)\n","\n","# Print best parameters and best score\n","print(\"Best Parameters:\", grid_search.best_params_)\n","print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n","\n","Output:- Best Parameters: {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n","Best Cross-Validation Accuracy: 0.9670\n","\n","Question 9: Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling. (Use Dataset from sklearn package) (Include your Python code and output in the code box below.)\n","Answer:- # Import libraries\n","import pandas as pd\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","data = load_breast_cancer()\n","\n","# Convert to DataFrame\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['target'] = data.target\n","\n","# Features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# ---------------- Without Scaling ----------------\n","model_no_scaling = LogisticRegression(max_iter=10000)\n","model_no_scaling.fit(X_train, y_train)\n","y_pred_no_scaling = model_no_scaling.predict(X_test)\n","acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n","\n","# ---------------- With Scaling ----------------\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","model_scaled = LogisticRegression(max_iter=10000)\n","model_scaled.fit(X_train_scaled, y_train)\n","y_pred_scaled = model_scaled.predict(X_test_scaled)\n","acc_scaled = accuracy_score(y_test, y_pred_scaled)\n","\n","# Print comparison\n","print(f\"Accuracy without Scaling: {acc_no_scaling:.4f}\")\n","print(f\"Accuracy with Scaling:    {acc_scaled:.4f}\")\n","\n","Output:- Accuracy without Scaling: 0.9561\n","Accuracy with Scaling:    0.9737\n","\n","Question 10: Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.\n","Answer:- Here’s how I’d approach the problem step-by-step, thinking as a data scientist who’s aware of the imbalanced nature of the dataset and the business goal (identifying likely responders without wasting marketing spend):\n","\n","1. Understanding the Problem\n","Goal: Predict probability that a customer will respond.\n","\n","\n","Challenge: Only 5% positives → highly imbalanced dataset.\n","\n","\n","Business focus: Missing responders (false negatives) is costly, but we also want to avoid wasting offers on uninterested customers (false positives).\n","\n","\n","\n","2. Data Handling\n","Data Cleaning: Handle missing values, remove duplicates, correct inconsistent entries.\n","\n","\n","Feature Engineering:\n","\n","\n","Create new features (e.g., recent purchase frequency, average spend, browsing patterns).\n","\n","\n","Encode categorical variables (One-Hot Encoding or Target Encoding).\n","\n","\n","Train-Test Split: Maintain original class imbalance during splitting (use stratify=y in train_test_split).\n","\n","\n","\n","3. Feature Scaling\n","Logistic regression works best when features are standardized.\n","\n","\n","Apply StandardScaler (mean=0, std=1) after splitting to avoid data leakage.\n","\n","\n","\n","4. Handling Class Imbalance\n","Since only 5% are positives:\n","Option 1: Class Weighting\n","\n","\n","Use LogisticRegression(class_weight='balanced') → assigns higher weight to minority class during training.\n","\n","\n","Option 2: Resampling\n","\n","\n","Oversample positives (e.g., SMOTE).\n","\n","\n","Undersample negatives (careful not to lose too much data).\n","\n","\n","Often combine weighting + slight oversampling.\n","\n","\n","\n","5. Model Building\n","Baseline model: Logistic regression with class_weight='balanced' to establish a reference performance.\n","\n","\n","Hyperparameter tuning:\n","\n","\n","C: Controls regularization strength (smaller → stronger regularization).\n","\n","\n","penalty: Try 'l1' (Lasso) for feature selection and 'l2' (Ridge) for stability.\n","\n","\n","Use GridSearchCV or RandomizedSearchCV with cross-validation while keeping class imbalance handling in place.\n","\n","\n","\n","6. Model Evaluation\n","Accuracy is misleading here because predicting all customers as non-responders gives 95% accuracy.\n"," Instead, focus on:\n","Precision, Recall, F1 Score (especially recall for the positive class).\n","\n","\n","Precision-Recall AUC (better than ROC-AUC for imbalanced problems).\n","\n","\n","Confusion Matrix to visualize trade-offs.\n","\n","\n","Business-specific threshold tuning:\n","\n","\n","Default threshold 0.5 may not be optimal.\n","\n","\n","Use validation curves to find the probability cutoff that maximizes business value (e.g., maximize recall while keeping precision above 30%).\n","\n","\n","\n","7. Deployment Considerations\n","Return probabilities instead of hard predictions so the marketing team can rank customers and target the top X% most likely to respond.\n","\n","\n","Monitor model drift — customer behavior changes over time.\n","\n","\n","Retrain periodically with fresh campaign data.\n","\n","\n","\n"],"metadata":{"id":"01MgMi6hTzsY"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"qddeyR5AR_cM","executionInfo":{"status":"error","timestamp":1755187112939,"user_tz":-330,"elapsed":31,"user":{"displayName":"Dhruv Sharma","userId":"13537995142501052598"}},"outputId":"4467b6b5-373b-43db-ad68-a9aed5fc90de"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (ipython-input-1569509835.py, line 38)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1569509835.py\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    Output:- Logistic Regression Model Accuracy: 0.956\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"E1_DXc3vTBd5"},"execution_count":null,"outputs":[]}]}